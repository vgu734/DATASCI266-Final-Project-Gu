{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c8f07e",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98708853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83da45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('helpermodule')\n",
    "\n",
    "from helpermodule import data\n",
    "from data import get_chapter_data, get_excerpt_data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e3a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module helpermodule.data in helpermodule:\n",
      "\n",
      "NAME\n",
      "    helpermodule.data\n",
      "\n",
      "FUNCTIONS\n",
      "    get_chapter_data()\n",
      "    \n",
      "    get_excerpt_data(n_words: int = 100)\n",
      "        #n_words: # words in an excerpt\n",
      "    \n",
      "    import_data()\n",
      "\n",
      "FILE\n",
      "    /home/vincentgu/DATASCI266-Final-Project-Gu/helpermodule/data.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8a6414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virginia\n",
      "“Citizens of the Solar Republic, this is your Sovereign.” I stare half blind into a ﬁring squad of ﬂy-eyed cameras. Out the viewport behind my stage, battle stations and ships of war ﬂoat beyond the \n"
     ]
    }
   ],
   "source": [
    "chapter_labels, chapter_examples = get_chapter_data()\n",
    "print(chapter_labels[0])\n",
    "print(chapter_examples[0][:199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8e3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "mottled with the resFlesh that has replaced the chunks Atlas took out. New metal ﬁngers extend from her knuckles. “Trouble?” she asks. “Pushy relations.” Without a smile, she turns back to watch the polar sky. Beyond the atmosphere of the planet, Atalantia’s warships rove, waiting for us to just nip our heads outside the great shield chains so they can drop mass drivers down and make craters of us. “Cold back here,” I say over the whistling wind. Our ship passes over the edge of an ice shelf. “Why don’t you head to mess? Colloway says it’s bad to sync\n"
     ]
    }
   ],
   "source": [
    "excerpt_labels, excerpt_examples = get_excerpt_data(n_words=100)\n",
    "print(excerpt_labels[100])\n",
    "print(excerpt_examples[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec6e3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n",
      "565\n",
      "2257\n",
      "565\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(excerpt_examples, excerpt_labels, test_size=.2, random_state=2457)\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158288e1",
   "metadata": {},
   "source": [
    "# BERT Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3b9bfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 21:34:57.106511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "import os\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68fff773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df066979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 21:35:53.534504: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c7a2e",
   "metadata": {},
   "source": [
    "## Basic BERT Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cfe2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf = tf.convert_to_tensor(x_train)\n",
    "x_test_tf = tf.convert_to_tensor(x_test)\n",
    "y_train_tf = tf.convert_to_tensor(y_train)\n",
    "y_test_tf = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20881c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples = 2000      # set number of train examples\n",
    "num_test_examples = 500        # set number of test examples\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 128                 # set max_length of the input sequence\n",
    "\n",
    "all_train_examples = [x.decode('utf-8') for x in x_train_tf.numpy()]\n",
    "all_test_examples = [x.decode('utf-8') for x in x_test_tf.numpy()]\n",
    "\n",
    "x_train_tf = bert_tokenizer(all_train_examples[:num_train_examples],\n",
    "              max_length=MAX_SEQUENCE_LENGTH,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_train_tf = y_train_tf[:num_train_examples]\n",
    "\n",
    "x_test_tf = bert_tokenizer(all_test_examples[:num_test_examples],\n",
    "              max_length=MAX_SEQUENCE_LENGTH,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_test_tf = y_test_tf[:num_test_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53727039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_classification_model(bert_model,\n",
    "                                     num_train_layers=0,\n",
    "                                     hidden_size = 200, \n",
    "                                     dropout=0.3,\n",
    "                                     learning_rate=0.00005):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes\n",
    "    \"\"\"\n",
    "    if num_train_layers == 0:\n",
    "        # Freeze all layers of pre-trained BERT model\n",
    "        bert_model.trainable = False\n",
    "\n",
    "    elif num_train_layers == 12: \n",
    "        # Train all layers of the BERT model\n",
    "        bert_model.trainable = True\n",
    "\n",
    "    else:\n",
    "        # Restrict training to the num_train_layers outer transformer layers\n",
    "        retrain_layers = []\n",
    "\n",
    "        for retrain_layer_number in range(num_train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "          \n",
    "        \n",
    "        print('retrain layers: ', retrain_layers)\n",
    "\n",
    "        for w in bert_model.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                #print('freezing: ', w)\n",
    "                w._trainable = False\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                   'token_type_ids': token_type_ids,\n",
    "                   'attention_mask': attention_mask}      \n",
    "\n",
    "    bert_out = bert_model(bert_inputs)\n",
    "\n",
    "    pooler_token = bert_out[1]\n",
    "    #cls_token = bert_out[0][:, 0, :]\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(pooler_token)\n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
    "\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "    \n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "                                 metrics='accuracy')\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ee471fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " attention_mask_layer (InputLay  [(None, 128)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_ids_layer (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids_layer (InputLay  [(None, 128)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['attention_mask_layer[0][0]',   \n",
      "                                thPoolingAndCrossAt               'input_ids_layer[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids_layer[0][0]']   \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)           (None, 200)          153800      ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 200)          0           ['hidden_layer[0][0]']           \n",
      "                                                                                                  \n",
      " classification_layer (Dense)   (None, 1)            201         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,464,273\n",
      "Trainable params: 108,464,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_classification_model = create_bert_classification_model(bert_model, num_train_layers=12)\n",
    "#confirm all layers are frozen\n",
    "bert_classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d67a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "bert_classification_model_history = bert_classification_model.fit(\n",
    "    [x_train_tf.input_ids, x_train_tf.token_type_ids, x_train_tf.attention_mask],\n",
    "    y_train_tf,\n",
    "    validation_data=([x_test_tf.input_ids, x_test_tf.token_type_ids, x_test_tf.attention_mask], y_test_tf),\n",
    "    batch_size=32,\n",
    "    epochs=2\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa96853",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
