{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c8f07e",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98708853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83da45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('helpermodule')\n",
    "\n",
    "from helpermodule import data\n",
    "from data import get_chapter_data, get_excerpt_data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e3a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module helpermodule.data in helpermodule:\n",
      "\n",
      "NAME\n",
      "    helpermodule.data\n",
      "\n",
      "FUNCTIONS\n",
      "    get_chapter_data()\n",
      "    \n",
      "    get_excerpt_data(n_words: int = 100)\n",
      "        #n_words: # words in an excerpt\n",
      "    \n",
      "    import_data()\n",
      "\n",
      "FILE\n",
      "    /home/vmgu/DATASCI266-Final-Project-Gu/helpermodule/data.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c8a6414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virginia\n",
      "“Citizens of the Solar Republic, this is your Sovereign.” I stare half blind into a ﬁring squad of ﬂy-eyed cameras. Out the viewport behind my stage, battle stations and ships of war ﬂoat beyond the \n"
     ]
    }
   ],
   "source": [
    "chapter_labels, chapter_examples = get_chapter_data()\n",
    "print(chapter_labels[0])\n",
    "print(chapter_examples[0][:199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8e3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darrow\n",
      "mottled with the resFlesh that has replaced the chunks Atlas took out. New metal ﬁngers extend from her knuckles. “Trouble?” she asks. “Pushy relations.” Without a smile, she turns back to watch the polar sky. Beyond the atmosphere of the planet, Atalantia’s warships rove, waiting for us to just nip our heads outside the great shield chains so they can drop mass drivers down and make craters of us. “Cold back here,” I say over the whistling wind. Our ship passes over the edge of an ice shelf. “Why don’t you head to mess? Colloway says it’s bad to sync\n"
     ]
    }
   ],
   "source": [
    "excerpt_labels, excerpt_examples = get_excerpt_data(n_words=100)\n",
    "print(excerpt_labels[100])\n",
    "print(excerpt_examples[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec6e3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n",
      "565\n",
      "2257\n",
      "565\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(excerpt_examples, excerpt_labels, test_size=.2, random_state=2457)\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158288e1",
   "metadata": {},
   "source": [
    "# BERT Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b9bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "import os\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68fff773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df066979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 09:35:21.514890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-04 09:35:21.549966: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-06-04 09:35:21.549982: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-06-04 09:35:21.550721: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c7a2e",
   "metadata": {},
   "source": [
    "## Basic BERT Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f9b3a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lyria', 'Darrow', 'Ephraim', 'Ephraim', 'Lysander']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train[:5])\n",
    "label = preprocessing.LabelEncoder()\n",
    "y = label.fit_transform(y_train)\n",
    "y = to_categorical(y)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfe2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf = tf.convert_to_tensor(x_train)\n",
    "x_test_tf = tf.convert_to_tensor(x_test)\n",
    "y_train_tf = tf.convert_to_tensor(y_train)\n",
    "y_test_tf = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20881c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples = 2000      # set number of train examples\n",
    "num_test_examples = 500        # set number of test examples\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 128                 # set max_length of the input sequence\n",
    "\n",
    "all_train_examples = [x.decode('utf-8') for x in x_train_tf.numpy()]\n",
    "all_test_examples = [x.decode('utf-8') for x in x_test_tf.numpy()]\n",
    "\n",
    "x_train_tf = bert_tokenizer(all_train_examples[:num_train_examples],\n",
    "              max_length=MAX_SEQUENCE_LENGTH,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_train_tf = y_train_tf[:num_train_examples]\n",
    "\n",
    "x_test_tf = bert_tokenizer(all_test_examples[:num_test_examples],\n",
    "              max_length=MAX_SEQUENCE_LENGTH,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_test_tf = y_test_tf[:num_test_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53727039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_classification_model(bert_model,\n",
    "                                     num_train_layers=0,\n",
    "                                     hidden_size = 200, \n",
    "                                     dropout=0.3,\n",
    "                                     learning_rate=0.00005):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes\n",
    "    \"\"\"\n",
    "    if num_train_layers == 0:\n",
    "        # Freeze all layers of pre-trained BERT model\n",
    "        bert_model.trainable = False\n",
    "\n",
    "    elif num_train_layers == 12: \n",
    "        # Train all layers of the BERT model\n",
    "        bert_model.trainable = True\n",
    "\n",
    "    else:\n",
    "        # Restrict training to the num_train_layers outer transformer layers\n",
    "        retrain_layers = []\n",
    "\n",
    "        for retrain_layer_number in range(num_train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "          \n",
    "        \n",
    "        print('retrain layers: ', retrain_layers)\n",
    "\n",
    "        for w in bert_model.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                #print('freezing: ', w)\n",
    "                w._trainable = False\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                   'token_type_ids': token_type_ids,\n",
    "                   'attention_mask': attention_mask}      \n",
    "\n",
    "    bert_out = bert_model(bert_inputs)\n",
    "\n",
    "    pooler_token = bert_out[1]\n",
    "    #cls_token = bert_out[0][:, 0, :]\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(pooler_token)\n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
    "\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "    \n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "                                 metrics='accuracy')\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee471fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_classification_model = create_bert_classification_model(bert_model, num_train_layers=12)\n",
    "#confirm all layers are frozen\n",
    "bert_classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classification_model_history = bert_classification_model.fit(\n",
    "    [x_train_tf.input_ids, x_train_tf.token_type_ids, x_train_tf.attention_mask],\n",
    "    y_train_tf,\n",
    "    validation_data=([x_test_tf.input_ids, x_test_tf.token_type_ids, x_test_tf.attention_mask], y_test_tf),\n",
    "    batch_size=32,\n",
    "    epochs=2\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa96853",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
